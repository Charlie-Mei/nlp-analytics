{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "by Charlie Mei cm3947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all text from the url, using code provided by lecturer in class 3 exercise\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pick a random news article (preferably with many entity mentions) from your Webhose dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first article from the Netflix Webhose dataset provided in Assignment 2\n",
    "url = 'https://www.stuff.co.nz/entertainment/tv-radio/300026661/13-reasons-why-the-popular-netflix-shows-creator-teases-chance-of-a-hopeful-ending'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all text from the webpage\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data = soup.findAll(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "National World Business Climate Change Sport Entertainment Life & Style Homed Travel Motoring Stuff Nation Play Stuff Quizzes Politics Premium Well & Good Food & Wine Parenting Rugby Farming Technology Opinion Auckland Wellington Canterbury Waikato Bay of Plenty Taranaki Manawatu Nelson Marlborough Timaru Otago Southland Careers Advertising Contact Privacy Â© 2020 Stuff Limited Entertainment TV & Radio 13 Reasons Why: The popular Netflix show's creator teases chance of a hopeful ending 14:49, Jun 03 2020 Facebook Twitter Whats App Reddit Email NETFLIX The final season of 13 Reasons Why is out. The controversial 13  Reasons Why is returning for its fourth and final season on Netflix from Friday and creator Brian Yorkey has indicated there will be a hopeful ending. Adapted from Jay Asher's 2007 novel, the show was released on Netflix in 2017 and began with the first season focused on the death of Hannah Baker, a 17-year-old American high school student who\n"
    }
   ],
   "source": [
    "text = text_from_html(html)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow directions to set up one of the Information Extraction services below, and write a Python program implementing API calls to extract Company/Organization and Geo entities from  the article chosen in Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to use ```SpaCy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Parse through text from webpage into a spacy nlp\n",
    "page = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Here are a list of companies/organizations referenced in the article: \n {'Health Ministry', 'Stuff Limited Entertainment TV & Radio', 'Premium Well & Good Food & Wine', 'Nelson Marlborough Timaru', 'Netflix', 'Ford', 'Mental Health Foundation', 'Entertainment Weekly', 'Yorkey'}\nHere are a list of geographies referenced in the article: \n {'US', 'Auckland', 'Australia', 'Netflix', 'Manawatu', 'North Star', 'Yorkey'}\n"
    }
   ],
   "source": [
    "# Entity label in spacy for company/organization and geo entities\n",
    "entity_labels = ['ORG', 'GPE']\n",
    "\n",
    "# Extract companies and geo entities from the article\n",
    "orgs = []\n",
    "geos = []\n",
    "for entity in page.ents:\n",
    "    if entity.label_ == 'ORG':\n",
    "        orgs.append(entity.text)\n",
    "    elif entity.label_ == 'GPE':\n",
    "        geos.append(entity.text)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(\"Here are a list of companies/organizations referenced in the article: \\n {}\".format(set(orgs)))\n",
    "print(\"Here are a list of geographies referenced in the article: \\n {}\".format(set(geos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download Crunchbase Open Data Map CSV file and store it in a directory on your computer\n",
    "\n",
    "### 4. Use the Class Exercise Jupyter Notebook as a reference to:\n",
    "- !pip install pyspark\n",
    "- load Crunchbase Open Data Map into notebook by modifying the path .csv(\".../...\") to the file on your computer where you stored the downloaded CSV file from Step 4.\n",
    "- find matches of Company or Organization entities identified in Step 3 using rlike function and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing spark to load the Couch database\n",
    "sc = SparkContext()\n",
    "config = sc.getConf()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "687755"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Load in the Couch DB\n",
    "df = sqlContext.read.option('header', 'true').option('delimiter', ',').option('inferSchema', 'true').csv('cb_odm_092419.csv')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Matches in the Couch DB for Health Ministry:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Stuff Limited Entertainment TV & Radio:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Premium Well & Good Food & Wine:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Nelson Marlborough Timaru:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Netflix:\n+--------------------+-------------------+--------------------+\n|     crunchbase_uuid|               name|     homepage_domain|\n+--------------------+-------------------+--------------------+\n|3a7ec450-5422-155...|            Netflix|         netflix.com|\n|9816bfd0-67cc-dea...|The Best of Netflix|thebestofnetflix.com|\n|e4ec497f-6c8f-3c4...|       Netflixology|    netflixology.com|\n|b909e0c1-7d24-85d...|    Netflix Reviews|  netflixreviews.net|\n|6769eb57-0fdf-451...|    Cookies4Netflix|        blogspot.com|\n+--------------------+-------------------+--------------------+\n\n\nMatches in the Couch DB for Ford:\n+--------------------+--------------------+--------------------+\n|     crunchbase_uuid|                name|     homepage_domain|\n+--------------------+--------------------+--------------------+\n|9249e8b6-409a-a80...|  Ford Motor Company|      forddirect.com|\n|08022867-4dc1-e22...|             Fordela|         fordela.com|\n|3b9ded42-da9e-1a0...|        Miracle Ford|   miraclefordtn.com|\n|ed74713f-5d95-628...|       Suburban Ford|    suburbanford.net|\n|13ccbaf0-5d42-3dd...|Fordex Industrial...| igiftswholesale.com|\n|57e027df-4f34-65d...|    Fordescapesydney|fordescapesydney....|\n|654e5736-4366-810...|  Ford Falcon Sydney|fordfalconsydney....|\n|096c20a4-b7fd-cc5...|  Ford Fiesta Sydney|fordfiestasydney....|\n|63614427-329f-76b...|  Ford Mondeo Sydney|fordmondeosydney....|\n|854f176e-f7ed-a29...|  Ford Ranger Sydney|fordrangersydney....|\n|6e5a06be-9140-89a...|Ford Territory Sy...|fordterritorysydn...|\n|01ba34dc-33c0-195...|        Thomson Ford|fordtransitsydney...|\n|ee612f5b-ab0e-7c6...|Thomson Ford NEW ...|  thomsonford.com.au|\n|dac49296-bce2-18c...|       Sinclair Ford| sinclairford.com.au|\n|776075fd-d926-ea9...| Barrhead Ford Sales|     barrheadford.ca|\n|d1887a63-2827-2ab...|       Ford Arospace|ford-engineering.com|\n|0cec4b68-326e-646...|British Leyland a...|british-leyland.c...|\n|60188862-de0d-c2e...|Merriman Curhan F...|      merrimanco.com|\n|0ee8cd71-bfd9-bf6...|     Ford Foundation|  fordfoundation.org|\n|b5811ed2-c30f-095...|    Towne Ford Sales|       towneford.com|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\nMatches in the Couch DB for Mental Health Foundation:\n+--------------------+--------------------+-------------------+\n|     crunchbase_uuid|                name|    homepage_domain|\n+--------------------+--------------------+-------------------+\n|b1bf4f5d-e5e0-b71...|Mental Health Fou...|mentalhealth.org.uk|\n+--------------------+--------------------+-------------------+\n\n\nMatches in the Couch DB for Entertainment Weekly:\n+--------------------+--------------------+---------------+\n|     crunchbase_uuid|                name|homepage_domain|\n+--------------------+--------------------+---------------+\n|f523d9c9-f482-d9b...|Entertainment Weekly|         ew.com|\n+--------------------+--------------------+---------------+\n\n\nMatches in the Couch DB for Yorkey:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\n"
    }
   ],
   "source": [
    "for org in set(orgs):\n",
    "    print('Matches in the Couch DB for {}:'.format(org))\n",
    "    match_df = df[df['name'].rlike(org)]\n",
    "    match_df['crunchbase_uuid', 'name', 'homepage_domain'].show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS\n",
    "\n",
    "Use the Class Exercise Jupyter Notebook as a reference to:\n",
    "- !pip install spacy \n",
    "- update TRAIN_DATA with annotations of entities (PERSON, LOCATION, or ORGANIZATION) from each sentence in the article selected in step 1\n",
    "- run spaCy_NER function to generate trained_nlp model\n",
    "- use trained_nlp to test entity recognition on another random news article from Webhose and print results to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat \n",
    "entity_labels = ['ORG', 'GPE']\n",
    "\n",
    "# Extract companies and geo entities from the article\n",
    "orgs = []\n",
    "geos = []\n",
    "for entity in page.ents:\n",
    "    if entity.label_ == 'ORG':\n",
    "        orgs.append(entity.text)\n",
    "    elif entity.label_ == 'GPE':\n",
    "        geos.append(entity.text)\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}