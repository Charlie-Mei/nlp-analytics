{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "by Charlie Mei cm3947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import spacy\n",
    "from spacy.gold import docs_to_json\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import SentenceSegmenter\n",
    "import random\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all body text from the url, using code provided by lecturer in class 3 exercise\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pick a random news article (preferably with many entity mentions) from your Webhose dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first article from the Netflix Webhose dataset provided in Assignment 2\n",
    "url = 'https://www.stuff.co.nz/entertainment/tv-radio/300026661/13-reasons-why-the-popular-netflix-shows-creator-teases-chance-of-a-hopeful-ending'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all body text from the webpage\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data = soup.findAll(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "National World Business Climate Change Sport Entertainment Life & Style Homed Travel Motoring Stuff Nation Play Stuff Quizzes Politics Premium Well & Good Food & Wine Parenting Rugby Farming Technology Opinion Auckland Wellington Canterbury Waikato Bay of Plenty Taranaki Manawatu Nelson Marlborough Timaru Otago Southland Careers Advertising Contact Privacy Â© 2020 Stuff Limited Entertainment TV & Radio 13 Reasons Why: The popular Netflix show's creator teases chance of a hopeful ending 14:49, Jun 03 2020 Facebook Twitter Whats App Reddit Email NETFLIX The final season of 13 Reasons Why is out. The controversial 13  Reasons Why is returning for its fourth and final season on Netflix from Friday and creator Brian Yorkey has indicated there will be a hopeful ending. Adapted from Jay Asher's 2007 novel, the show was released on Netflix in 2017 and began with the first season focused on the death of Hannah Baker, a 17-year-old American high school student who\n"
    }
   ],
   "source": [
    "text = text_from_html(html)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow directions to set up one of the Information Extraction services below, and write a Python program implementing API calls to extract Company/Organization and Geo entities from  the article chosen in Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to use ```SpaCy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Parse through text from webpage into a spacy nlp\n",
    "page = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Here are a list of companies/organizations referenced in the article: \n {'Premium Well & Good Food & Wine', 'Mental Health Foundation', 'Entertainment Weekly', 'Urban', 'Stuff Limited Entertainment TV & Radio', 'Super Rugby Aotearoa', 'Nelson Marlborough Timaru', 'Netflix', 'Yorkey'}\nHere are a list of geographies referenced in the article: \n {'Victoria', 'Auckland', 'Australia', 'Netflix', \"New Zealand's\", 'North Star', 'Wellington', 'Manawatu', 'Yorkey'}\n"
    }
   ],
   "source": [
    "# Entity label in spacy for company/organization and geo entities\n",
    "entity_labels = ['ORG', 'GPE']\n",
    "\n",
    "# Extract companies and geo entities from the article\n",
    "orgs = []\n",
    "geos = []\n",
    "for entity in page.ents:\n",
    "    if entity.label_ == 'ORG':\n",
    "        orgs.append(entity.text)\n",
    "    elif entity.label_ == 'GPE':\n",
    "        geos.append(entity.text)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(\"Here are a list of companies/organizations referenced in the article: \\n {}\".format(set(orgs)))\n",
    "print(\"Here are a list of geographies referenced in the article: \\n {}\".format(set(geos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download Crunchbase Open Data Map CSV file and store it in a directory on your computer\n",
    "\n",
    "### 4. Use the Class Exercise Jupyter Notebook as a reference to:\n",
    "- !pip install pyspark\n",
    "- load Crunchbase Open Data Map into notebook by modifying the path .csv(\".../...\") to the file on your computer where you stored the downloaded CSV file from Step 4.\n",
    "- find matches of Company or Organization entities identified in Step 3 using rlike function and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing spark to load the Couch database\n",
    "sc = SparkContext()\n",
    "config = sc.getConf()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "687755"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Load in the Couch DB\n",
    "df = sqlContext.read.option('header', 'true').option('delimiter', ',').option('inferSchema', 'true').csv('cb_odm_092419.csv')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Matches in the Couch DB for Premium Well & Good Food & Wine:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Mental Health Foundation:\n+--------------------+--------------------+-------------------+\n|     crunchbase_uuid|                name|    homepage_domain|\n+--------------------+--------------------+-------------------+\n|b1bf4f5d-e5e0-b71...|Mental Health Fou...|mentalhealth.org.uk|\n+--------------------+--------------------+-------------------+\n\n\nMatches in the Couch DB for Entertainment Weekly:\n+--------------------+--------------------+---------------+\n|     crunchbase_uuid|                name|homepage_domain|\n+--------------------+--------------------+---------------+\n|f523d9c9-f482-d9b...|Entertainment Weekly|         ew.com|\n+--------------------+--------------------+---------------+\n\n\nMatches in the Couch DB for Urban:\n+--------------------+--------------------+--------------------+\n|     crunchbase_uuid|                name|     homepage_domain|\n+--------------------+--------------------+--------------------+\n|86badc3f-db74-5b8...|       Urban Mapping|    urbanmapping.com|\n|3009e749-26a7-6d0...|    Urban Dictionary| urbandictionary.com|\n|ebaf7353-5661-afa...|          Urbanspoon|      urbanspoon.com|\n|e2d83ed4-4eec-f8f...|        Urbantastic!|     urbantastic.com|\n|2925e479-b05f-b63...|          TradeUrban|      tradeurban.com|\n|27196840-28cb-1db...|           Urban One|          urban1.com|\n|7cccc007-2fa2-761...|           UrbanKite|       urbankite.com|\n|9d52fa1e-2074-c60...|Sub Urban Media G...|      theblvdmag.com|\n|7fc749a0-36e1-404...|       Urban Interns|    urbaninterns.com|\n|7d256d0d-71f7-538...|        Peixe Urbano|  peixeurbano.com.br|\n|b51190e4-7a3a-5b3...|          BuzzUrbano|      buzzurbano.com|\n|a570be74-c768-88d...|         Urbansocial|     urbansocial.com|\n|529e729e-fed6-aff...|            Urban FT|         urbanft.com|\n|c8a25ede-fd65-f6e...|        Urban Spoils|     urbanspoils.com|\n|c70fc4d0-cf17-a73...|Elastic Urban Tru...|         elastic.com|\n|8ef73a24-39b7-6aa...|       Urban Escapes| urbanescapesnyc.com|\n|57ac0578-fd19-289...|    UrbanJunkies.com|    urbanjunkies.com|\n|233a6348-549c-8f9...|Urban Communicati...|        ourspace.com|\n|af1d2a1d-bf24-db0...|        Hotel Urbano|     hotelurbano.com|\n|a60b1c42-a394-a1c...|Urban Fiesta Weekend|urbanfiestaweeken...|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\nMatches in the Couch DB for Stuff Limited Entertainment TV & Radio:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Super Rugby Aotearoa:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Nelson Marlborough Timaru:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Netflix:\n+--------------------+-------------------+--------------------+\n|     crunchbase_uuid|               name|     homepage_domain|\n+--------------------+-------------------+--------------------+\n|3a7ec450-5422-155...|            Netflix|         netflix.com|\n|9816bfd0-67cc-dea...|The Best of Netflix|thebestofnetflix.com|\n|e4ec497f-6c8f-3c4...|       Netflixology|    netflixology.com|\n|b909e0c1-7d24-85d...|    Netflix Reviews|  netflixreviews.net|\n|6769eb57-0fdf-451...|    Cookies4Netflix|        blogspot.com|\n+--------------------+-------------------+--------------------+\n\n\nMatches in the Couch DB for Yorkey:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\n"
    }
   ],
   "source": [
    "for org in set(orgs):\n",
    "    print('Matches in the Couch DB for {}:'.format(org))\n",
    "    match_df = df[df['name'].rlike(org)]\n",
    "    match_df['crunchbase_uuid', 'name', 'homepage_domain'].show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS\n",
    "\n",
    "Use the Class Exercise Jupyter Notebook as a reference to:\n",
    "- !pip install spacy \n",
    "- update TRAIN_DATA with annotations of entities (PERSON, LOCATION, or ORGANIZATION) from each sentence in the article selected in step 1\n",
    "- run spaCy_NER function to generate trained_nlp model\n",
    "- use trained_nlp to test entity recognition on another random news article from Webhose and print results to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Homed Travel Motoring Stuff Nation Play Stuff Quizzes Politics Premium Well & Good Food & Wine"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Break up into body text into sentences\n",
    "sentences = [sentence for sentence in page.sents]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities from each sentence\n",
    "relevant_entities = ['GPE', 'ORG', 'PERSON']\n",
    "\n",
    "def extractEntityInfo(sent, entities):\n",
    "    ents = []\n",
    "    for ent in sent.ents:\n",
    "        if ent.label_ in entities:\n",
    "            ents.append((ent.start_char, ent.end_char, ent.label_))\n",
    "        else:\n",
    "            continue\n",
    "    return (str(sent), {'entities': ents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('Homed Travel Motoring Stuff Nation Play Stuff Quizzes Politics Premium Well & Good Food & Wine',\n {'entities': [(168, 199, 'ORG')]})"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "extractEntityInfo(sentences[1], relevant_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('                                 National World Business Climate Change Sport Entertainment Life & Style',\n  {'entities': []}),\n ('Homed Travel Motoring Stuff Nation Play Stuff Quizzes Politics Premium Well & Good Food & Wine',\n  {'entities': [(168, 199, 'ORG')]}),\n ('Parenting Rugby Farming Technology Opinion Auckland Wellington Canterbury Waikato Bay of Plenty',\n  {'entities': [(243, 251, 'GPE')]}),\n ('Taranaki Manawatu Nelson Marlborough Timaru',\n  {'entities': [(305, 313, 'GPE'), (314, 339, 'ORG')]}),\n ('Otago Southland Careers', {'entities': []})]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Build the training dataset\n",
    "TRAIN_DATA = []\n",
    "for sentence in sentences:\n",
    "    TRAIN_DATA.append(extractEntityInfo(sentence, relevant_entities))\n",
    "TRAIN_DATA[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build NER training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a NER training model, adapted from lecture slides\n",
    "def train_spacy(data, iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')\n",
    "\n",
    "    # Add NER trainer to pipe\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    \n",
    "    # Add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # Disable all other pipes and just train NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting iteration 0\n{'ner': 114.39401927567909}\nStarting iteration 1\n{'ner': 1.1293316454665398e-16}\nStarting iteration 2\n{'ner': 6.377145992719412e-18}\nStarting iteration 3\n{'ner': 1.4138139394281897e-16}\nStarting iteration 4\n{'ner': 8.626094947986659e-17}\nStarting iteration 5\n{'ner': 4.309852801873665e-16}\nStarting iteration 6\n{'ner': 1.0347297006906193e-16}\nStarting iteration 7\n{'ner': 1.2138217501899623e-16}\nStarting iteration 8\n{'ner': 2.0081938945163972e-17}\nStarting iteration 9\n{'ner': 2.900061819191236e-17}\n"
    }
   ],
   "source": [
    "prdnlp = train_spacy(TRAIN_DATA, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test trained model on new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all body text from another webpage from webhose dataset from above\n",
    "url2 = 'https://www.stuff.co.nz/entertainment/entertainment-top-stories/300026292/judge-gives-control-of-tiger-king-joe-exotics-zoo-to-carole-baskin'\n",
    "\n",
    "html = request.urlopen(url2).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data = soup.findAll(text=True)\n",
    "\n",
    "text2 = text_from_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model\n",
    "test_doc = prdnlp(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in test_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}