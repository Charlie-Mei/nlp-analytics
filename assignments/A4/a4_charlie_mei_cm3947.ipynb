{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "by Charlie Mei cm3947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import spacy\n",
    "from spacy.gold import docs_to_json\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import SentenceSegmenter\n",
    "import random\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all text from the url, using code provided by lecturer in class 3 exercise\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pick a random news article (preferably with many entity mentions) from your Webhose dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first article from the Netflix Webhose dataset provided in Assignment 2\n",
    "url = 'https://www.stuff.co.nz/entertainment/tv-radio/300026661/13-reasons-why-the-popular-netflix-shows-creator-teases-chance-of-a-hopeful-ending'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all text from the webpage\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data = soup.findAll(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "National World Business Climate Change Sport Entertainment Life & Style Homed Travel Motoring Stuff Nation Play Stuff Quizzes Politics Premium Well & Good Food & Wine Parenting Rugby Farming Technology Opinion Auckland Wellington Canterbury Waikato Bay of Plenty Taranaki Manawatu Nelson Marlborough Timaru Otago Southland Careers Advertising Contact Privacy © 2020 Stuff Limited Entertainment TV & Radio 13 Reasons Why: The popular Netflix show's creator teases chance of a hopeful ending 14:49, Jun 03 2020 Facebook Twitter Whats App Reddit Email NETFLIX The final season of 13 Reasons Why is out. The controversial 13  Reasons Why is returning for its fourth and final season on Netflix from Friday and creator Brian Yorkey has indicated there will be a hopeful ending. Adapted from Jay Asher's 2007 novel, the show was released on Netflix in 2017 and began with the first season focused on the death of Hannah Baker, a 17-year-old American high school student who\n"
    }
   ],
   "source": [
    "text = text_from_html(html)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow directions to set up one of the Information Extraction services below, and write a Python program implementing API calls to extract Company/Organization and Geo entities from  the article chosen in Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to use ```SpaCy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Parse through text from webpage into a spacy nlp\n",
    "page = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Here are a list of companies/organizations referenced in the article: \n {'Bloomfield Insurance', 'Premium Well & Good Food & Wine', 'Stuff Limited Entertainment TV & Radio', 'Navy', 'Mental Health Foundation', 'Yorkey', 'Entertainment Weekly', 'IAG', 'National', 'Nelson Marlborough Timaru', 'AMI', 'Netflix'}\nHere are a list of geographies referenced in the article: \n {'Yorkey', 'Auckland', 'North Star', 'Manawatu', 'New Zealand', 'Virgin Australia', 'Netflix'}\n"
    }
   ],
   "source": [
    "# Entity label in spacy for company/organization and geo entities\n",
    "entity_labels = ['ORG', 'GPE']\n",
    "\n",
    "# Extract companies and geo entities from the article\n",
    "orgs = []\n",
    "geos = []\n",
    "for entity in page.ents:\n",
    "    if entity.label_ == 'ORG':\n",
    "        orgs.append(entity.text)\n",
    "    elif entity.label_ == 'GPE':\n",
    "        geos.append(entity.text)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(\"Here are a list of companies/organizations referenced in the article: \\n {}\".format(set(orgs)))\n",
    "print(\"Here are a list of geographies referenced in the article: \\n {}\".format(set(geos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download Crunchbase Open Data Map CSV file and store it in a directory on your computer\n",
    "\n",
    "### 4. Use the Class Exercise Jupyter Notebook as a reference to:\n",
    "- !pip install pyspark\n",
    "- load Crunchbase Open Data Map into notebook by modifying the path .csv(\".../...\") to the file on your computer where you stored the downloaded CSV file from Step 4.\n",
    "- find matches of Company or Organization entities identified in Step 3 using rlike function and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing spark to load the Couch database\n",
    "sc = SparkContext()\n",
    "config = sc.getConf()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "687755"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Load in the Couch DB\n",
    "df = sqlContext.read.option('header', 'true').option('delimiter', ',').option('inferSchema', 'true').csv('cb_odm_092419.csv')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Matches in the Couch DB for Bloomfield Insurance:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Premium Well & Good Food & Wine:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Stuff Limited Entertainment TV & Radio:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Navy:\n+--------------------+--------------------+--------------------+\n|     crunchbase_uuid|                name|     homepage_domain|\n+--------------------+--------------------+--------------------+\n|4e4d3760-f1b5-4d6...|            Old Navy|         oldnavy.com|\n|b7a505ff-ed92-7e0...|      America's Navy|            navy.com|\n|d5390ae3-43c8-7a3...|  United States Navy|            navy.mil|\n|3c485f8a-b73b-5e9...|Navy Federal Cred...|     navyfederal.org|\n|06e84337-d5e9-24e...|Uncle Sams Army N...|   armynavydeals.com|\n|6dbf45d3-13f8-005...|          Royal Navy|    royalnavy.mod.uk|\n|e5e5cc5b-f9d5-9a7...|        Israeli Navy|              idf.il|\n|153945ea-36e2-20f...|     Fish Navy Films|        fishnavy.com|\n|659b4133-ae3d-c55...|Navy Exchange Ser...|  mynavyexchange.com|\n|cb0e6ce8-d618-319...|    Royal Dutch Navy|  netherlandsnavy.nl|\n|8fd13eb0-5dd3-3fe...|             NetNavy|         netnavy.org|\n|24fee339-8498-05b...|            NavyDuck|        navyduck.com|\n|2f395499-3b2f-891...|         Navy’s AT&L|             osd.mil|\n|5cc2d7ff-e206-95f...|          Navy Times|       navytimes.com|\n|b890e67d-d2ba-15e...|Navy Electronics ...|            empf.org|\n|98af2e3d-b47f-003...|        Navy Veteran|        navyvets.com|\n|cfdc04bc-fee8-b1d...|Royal Australian ...|         navy.gov.au|\n|eb0e8b82-ce2b-3c2...|Navyautotransport.co|navyautotransport.co|\n|98f4046d-c193-d64...|         Navy Movers|       navymovers.co|\n|d380a20e-3445-d9d...|        Navy Digital|         navycph.com|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\nMatches in the Couch DB for Mental Health Foundation:\n+--------------------+--------------------+-------------------+\n|     crunchbase_uuid|                name|    homepage_domain|\n+--------------------+--------------------+-------------------+\n|b1bf4f5d-e5e0-b71...|Mental Health Fou...|mentalhealth.org.uk|\n+--------------------+--------------------+-------------------+\n\n\nMatches in the Couch DB for Yorkey:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for Entertainment Weekly:\n+--------------------+--------------------+---------------+\n|     crunchbase_uuid|                name|homepage_domain|\n+--------------------+--------------------+---------------+\n|f523d9c9-f482-d9b...|Entertainment Weekly|         ew.com|\n+--------------------+--------------------+---------------+\n\n\nMatches in the Couch DB for IAG:\n+--------------------+--------------------+------------------+\n|     crunchbase_uuid|                name|   homepage_domain|\n+--------------------+--------------------+------------------+\n|726e4d34-7617-20f...|          EVIAGENICS|    eviagenics.com|\n|fa074420-cbbd-4a9...|        IAG Research|iagresearch.com.au|\n|faafe81c-e118-86e...|  EPINEX DIAGNOSTICS|        epinex.com|\n|c9650464-6c92-32b...|Insurance Austral...|        iag.com.au|\n|ed5390bd-9ea5-784...|J O O N - PHOTOGR...|      sumodori.com|\n|93c18554-a202-5cf...|   LYZER DIAGNOSTICS|  eveiamedical.com|\n|78ee8c54-1e49-1b5...|TARGETED DIAGNOST...|        tdtinc.com|\n|fe9d2a20-e873-dea...| PIAGGI Art & Living|      piaggi.co.uk|\n|f473682e-4020-bb9...|                IAGO|         iago.chat|\n|7fb8f898-6463-48e...|          IL MAKIAGE|     ilmakiage.com|\n|4836eb25-9ff9-4a4...|             IAGENTE|    iagente.com.br|\n|9a70ff00-47bc-476...|                 IAG|        iag.com.au|\n|056f201b-6eae-451...|      DNA DIAGNOSTIC|dna-diagnostic.com|\n+--------------------+--------------------+------------------+\n\n\nMatches in the Couch DB for National:\n+--------------------+--------------------+--------------------+\n|     crunchbase_uuid|                name|     homepage_domain|\n+--------------------+--------------------+--------------------+\n|43e6b569-64a1-48b...|     National Banana|  nationalbanana.com|\n|354d9012-b362-9a5...| National Geographic|nationalgeographi...|\n|f6af63c6-7a24-5fe...|National Leisure ...|             nlg.com|\n|d28341dc-8c15-bf3...|National Transcri...|transcriptcenter.com|\n|f4d6ef0f-16cb-d76...|National Semicond...|        national.com|\n|ca5049cf-89ef-0f0...|    National Lampoon| nationallampoon.com|\n|ca5049cf-89ef-0f0...|    National Lampoon| nationallampoon.com|\n|4ac03e1c-3222-a99...|National Associat...|            napw.com|\n|efe05462-f23c-74b...|National Defensiv...|defensivedriverse...|\n|70bd3e77-cd5b-43c...|National Business...|       natbusbro.com|\n|2ab76ad3-feca-8e2...|Access National Bank|accessnationalban...|\n|8f0bdd1c-64a8-e24...|         AdsNational|     adsnational.com|\n|84d8ed07-2c68-41a...|National Rural Te...|           nrtc.coop|\n|1918596f-2fde-42d...|National Credit R...|nationalcreditrep...|\n|d3dbbb07-0df0-efd...|National Interest...|        nisc-llc.com|\n|d3008896-4003-e4a...|  National Cinemedia|             ncm.com|\n|b3c81427-8b5f-13c...|National Diagnostics|        natldiag.com|\n|955e122b-489e-fef...|Los Alamos Nation...|            lanl.gov|\n|32582893-43d1-f1f...|National Growth F...|  ngf-sanantonio.com|\n|83d957e2-6337-3c3...|National Business...|           nbrii.com|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\nMatches in the Couch DB for Nelson Marlborough Timaru:\n+---------------+----+---------------+\n|crunchbase_uuid|name|homepage_domain|\n+---------------+----+---------------+\n+---------------+----+---------------+\n\n\nMatches in the Couch DB for AMI:\n+--------------------+--------------------+--------------------+\n|     crunchbase_uuid|                name|     homepage_domain|\n+--------------------+--------------------+--------------------+\n|e63b69b4-e699-ed7...|        AMI Software|           amisw.com|\n|3227bae6-5c13-7ac...|        WAMI Concept|    wami-concept.com|\n|24cfe061-41a6-4e7...|              NAMICS|        namics.co.jp|\n|23825a22-212b-b31...|SEO Agency DYNAMI...|         dynamind.at|\n|b537f0e7-a33f-6fe...|   MIAMIBEACHCPA LLC|   miamibeachcpa.com|\n|db4cf46d-7129-a37...|           GAMINSIDE|       gaminside.com|\n|b90c3ba4-2a3c-a68...|METAMINDS Consulting|            mmgs.com|\n|b8541133-3310-57a...|             ORIGAMI|origami-creative.com|\n|d140b965-f1e8-666...|HBA Inc. (AMIC Gr...|         hba-inc.com|\n|5be33d57-0347-9d6...|        AMIA Systems|    amia-systems.com|\n|2d634f21-0692-1a5...|         SAMI Health|      samihealth.com|\n|5ddc0865-7e31-4e2...| AMI Healthcare, Inc|   amihealthcare.net|\n|1fb15bc8-ea58-723...|            CAMINTEL|   camintelgroup.com|\n|9a0ab617-00a0-723...|AMI Entertainment...|    taptvtonight.com|\n|9a6723c1-b0e4-414...| AMIS Holdings, Inc.|            amis.com|\n|fc064c1b-5ddb-5e7...|       SAIFSAMIR.COM|       saifsamir.com|\n|0927a09e-2dc6-35e...|            ENCAMINA|        encamina.com|\n|94399dd7-1ed0-fe5...| CREAMIVE Web Agency|        creamive.com|\n|5ce9c634-2c84-eda...|              CAMICO|          camico.com|\n|c66ff19c-2d5c-c7f...|             BITAMIN|     realbitamin.com|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\nMatches in the Couch DB for Netflix:\n+--------------------+-------------------+--------------------+\n|     crunchbase_uuid|               name|     homepage_domain|\n+--------------------+-------------------+--------------------+\n|3a7ec450-5422-155...|            Netflix|         netflix.com|\n|9816bfd0-67cc-dea...|The Best of Netflix|thebestofnetflix.com|\n|e4ec497f-6c8f-3c4...|       Netflixology|    netflixology.com|\n|b909e0c1-7d24-85d...|    Netflix Reviews|  netflixreviews.net|\n|6769eb57-0fdf-451...|    Cookies4Netflix|        blogspot.com|\n+--------------------+-------------------+--------------------+\n\n\n"
    }
   ],
   "source": [
    "for org in set(orgs):\n",
    "    print('Matches in the Couch DB for {}:'.format(org))\n",
    "    match_df = df[df['name'].rlike(org)]\n",
    "    match_df['crunchbase_uuid', 'name', 'homepage_domain'].show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS\n",
    "\n",
    "Use the Class Exercise Jupyter Notebook as a reference to:\n",
    "- !pip install spacy \n",
    "- update TRAIN_DATA with annotations of entities (PERSON, LOCATION, or ORGANIZATION) from each sentence in the article selected in step 1\n",
    "- run spaCy_NER function to generate trained_nlp model\n",
    "- use trained_nlp to test entity recognition on another random news article from Webhose and print results to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'id': 0,\n 'paragraphs': [{'raw': 'I like London',\n   'sentences': [{'tokens': [{'id': 0,\n       'orth': 'I',\n       'tag': 'PRP',\n       'head': 1,\n       'dep': 'nsubj',\n       'ner': 'O'},\n      {'id': 1,\n       'orth': 'like',\n       'tag': 'VBP',\n       'head': 0,\n       'dep': 'ROOT',\n       'ner': 'O'},\n      {'id': 2,\n       'orth': 'London',\n       'tag': 'NNP',\n       'head': -1,\n       'dep': 'dobj',\n       'ner': 'U-GPE'}],\n     'brackets': []}],\n   'cats': []}]}"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "doc = nlp(\"I like London\")\n",
    "docs_to_json([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a NER training model, adapted from lecture slides\n",
    "def train_spacy(data, iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')\n",
    "\n",
    "    # Add NER trainer to pipe\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    \n",
    "    # Add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # Disable all other pipes and just train NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}