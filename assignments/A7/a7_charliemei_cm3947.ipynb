{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595187435276",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7\n",
    "by Charlie Mei cm2947\n",
    "\n",
    "Using the Week 7 Class Exercise as a reference, write a Python program that filters out exactly and/or semantically duplicate articles from your Webhose dataset of news articles:\n",
    "\n",
    "- Use LSH (SimHash or MinHash), separately or along with Word2Vec, to deduplicate your Webhose feeds based on titles\n",
    "- Make sure to store entire feeds in a JSON, text or CSV file\n",
    "\n",
    "Your final submission should include the Jupyter Notebook with the original set of titles and a deduplicated subset of titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim, operator\n",
    "from gensim.models import KeyedVectors\n",
    "from simhash import Simhash, SimhashIndex\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'webhose_apple.json'\n",
    "WORD_2_VEC_DIR = r'C:\\Github\\nlp-analytics'\n",
    "W2V_FILE = r'\\GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in JSON object as a list of Python dictionaries\n",
    "def parse_json_file(json_file):\n",
    "    with open(json_file) as f:\n",
    "        json_parsed = f.readlines()\n",
    "    feeds = [json.loads(feed) for feed in json_parsed]\n",
    "    return feeds\n",
    "\n",
    "\n",
    "def load_wordvec_model(modelName, modelFile, flagBin, model_path):\n",
    "    print('Loading ' + modelName + ' model...')\n",
    "    model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
    "    print('Finished loading ' + modelName + ' model...')\n",
    "    return model\n",
    "\n",
    "# function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    \n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output\n",
    "\n",
    "# function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output\n",
    "\n",
    "def cleanup(input):\n",
    "    # remove English stopwords\n",
    "    input = input.replace(\"'s\", \" \").replace(\"n’t\", \" not\").replace(\"’ve\", \" have\")\n",
    "    input = re.sub(r'[^a-zA-Z0-9 ]', '', input)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading Word2Vec model...\nFinished loading Word2Vec model...\n"
    }
   ],
   "source": [
    "w2v_model = load_wordvec_model('Word2Vec', W2V_FILE, True, WORD_2_VEC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Webhose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "webhose_data = parse_json_file(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will base deduplication on article titles only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of news feed titles and include an ID flag for each feed in the original data\n",
    "feeds = []\n",
    "index = 0\n",
    "for feed in webhose_data:\n",
    "    feeds.append(feed['title'])\n",
    "    # Add index to original feeds dataset\n",
    "    feed['id'] = index\n",
    "    index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a simhash logger\n",
    "logging.getLogger('simhash').setLevel(logging.CRITICAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simhashed object\n",
    "obj = [(str(feed['id']), Simhash(str(feed['title']))) for feed in webhose_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simhash_index(obj, dist):\n",
    "    print('Generating Simhash Index based on maximum hemming distance of {}...'.format(dist))\n",
    "    index = SimhashIndex(obj, k=dist)\n",
    "    return index\n",
    "\n",
    "def length_of_simhash_dupes(selection_index, index, feeds):\n",
    "    selected_feed = feeds[selection_index]\n",
    "    feed_hash = Simhash(str(selected_feed['title']))\n",
    "    dupe_indices = index.get_near_dups(feed_hash)\n",
    "    return dupe_indices\n",
    "\n",
    "def length_of_w2v_dupes(selection_index, dupe_indices, feeds, threshold_score=0.7):\n",
    "    count_dupe = 0\n",
    "    removed_dupes = []\n",
    "    \n",
    "    for dupe in dupe_indices:\n",
    "        try:\n",
    "            score = calc_similarity(feeds[selection_index]['title'], feeds[int(dupe)]['title'], w2v_model)\n",
    "        except:\n",
    "            score = 0\n",
    "        \n",
    "        if score > threshold_score:\n",
    "            count_dupe += 1\n",
    "            removed_dupes.append(dupe)\n",
    "    \n",
    "    return [count_dupe, removed_dupes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Hemming distance and threshold scores for deduplication\n",
    "\n",
    "I will test what the optimal Hemming distance and scoring threhold score to use based on the performance on one randomly selected article in the dataset. I will first change the Hemming distance, then adjust the threshold score such that the resulting list of articles are only the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feed = feeds[100]\n",
    "\n",
    "# Function to test dupes on test article\n",
    "def print_dupes(dist, score):\n",
    "    # Generate a SimHash based on specified Hemming distance\n",
    "    simhash_index = generate_simhash_index(obj, dist=dist)\n",
    "\n",
    "    # Generate list of dupes from SimHash\n",
    "    dupe_indices = length_of_simhash_dupes(100, simhash_index, webhose_data)\n",
    "\n",
    "    # Generate filtered SimHash list using Word2Vec\n",
    "    count_dupe, removed_dupes = length_of_w2v_dupes(100, dupe_indices, webhose_data, threshold_score=score)\n",
    "\n",
    "    # Print out the resulting duplicates\n",
    "    for dupe in removed_dupes:\n",
    "        print(webhose_data[int(dupe)]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Generating Simhash Index based on maximum hemming distance of 10...\nThe Next Apple Pencil Could Come In A Black Finish\n"
    }
   ],
   "source": [
    "print_dupes(10, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps restrictive - let's increase Hemming distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Generating Simhash Index based on maximum hemming distance of 20...\nNext-Generation Apple Pencil Rumoured to Come in Black Colour\nApple Pencil Could Be Released in Black\nThe Next Apple Pencil Could Come In A Black Finish\n"
    }
   ],
   "source": [
    "print_dupes(20, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Generating Simhash Index based on maximum hemming distance of 30...\nThis Year’s Apple MacBook To Come With ARM-based Chip\nHow To Watch 'The Great' In The UK\nSlap An Apple Watch Series 3 Onto Your Wrist For Just $179 If You’re Quick\nApple AirPods Might Get An Ambient Light Sensor, Here's What It Would Do\nA $50 Apple Watch is Possible… But Not From Apple\nMartin Scorsese’s Next Film Is Coming to Apple TV+\nA Full-Length ‘Fraggle Rock’ (Clap Clap) Reboot Is Coming to Apple TV+\nHow To Sign Up For HBO Max So You Can Watch 'Friends' All Day\nThe Next Apple Pencil Could Come In A Black Finish\nApple Pencil Could Be Released in Black\n‘The Office’ Would Have Been Cancelled After Season 1 If It Weren’t For Apple\nApple iOS 13.5 Update To Fix Face ID Unlock Issue When Wearing A Mask & More\nCould Apple Be Worth $2 Trillion in Just 4 Years? | The Motley Fool\nNext-Generation Apple Pencil Rumoured to Come in Black Colour\nWith The New iOS 13.5 You Can Unlock Your iPhone While Wearing a Face Mask\nIf I Could Buy Only 1 Warren Buffett Stock Right Now, This Would Be It | The Motley Fool\nApple Is Expected To Release A larger iPhone 12 Pro Later This Year\nThe #1 Reason To Get An iPad Pro – If You Play Guitar…\nHow to Get Apple Watch to Announce Time with Any Watch Face\nThe New 13-Inch MacBook Pro's Keyboard Really Is That Good\nApple’s First ARM-Based MacBook Could Be Arriving Later This Year\nBlackout Tuesday: What Is It And Why Is Instagram Covered In Black Squares?\nToday in Gear: The Leather Strap Your Apple Watch Needs, a Fun Detail on the Upcoming Hummer & More\nApple Stores Are Reopening This Week: See If Your Store Is One of Them!\n"
    }
   ],
   "source": [
    "print_dupes(30, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the Hemming distance is too large. How about changing the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Generating Simhash Index based on maximum hemming distance of 20...\niPhone 12 May Not Come Bundled With Free Headphones\nApple To Reopen 100 Stores This Week\nRealme Buds Air vs Buds Air Neo: Which one should you go for? - Technology News\nNew rumor says that the next generation of Apple Pencil will come in black\nNext-Generation Apple Pencil Rumoured to Come in Black Colour\nApple Watch Series 3 38mm GPS Aluminum Smart Watch w/ Sport Band $179 at Amazon\niPhone 12 May Not Come Bundled With Free Headphones\nRumor claims future Apple Pencil will come in black\nThree States Will Start Using Google and Apple COVID-19 Contact-Tracing Tech\nApple Pencil Could Be Released in Black\nApple to Reopen Two Apple Store Location in Japan This Week\nThe Next Apple Pencil Could Come In A Black Finish\nApple Reportedly to Reopen 130 of Its 271 U.S. Stores This Week\n"
    }
   ],
   "source": [
    "print_dupes(20, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold now is too low. My optimal parameters are: Hemming distance of 20 and using threhold score of 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-scale deduplication (```hemming_distance=20``` and ```threshold_score=0.7```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Generating Simhash Index based on maximum hemming distance of 20...\n"
    }
   ],
   "source": [
    "hemming_distance = 20\n",
    "simhash_index = generate_simhash_index(obj, hemming_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test dupes on test article\n",
    "def find_dupes(ind, simhash_index, webhose_data):\n",
    "    dupe_indices = length_of_simhash_dupes(ind, simhash_index, webhose_data)\n",
    "\n",
    "    # Generate filtered SimHash list using Word2Vec\n",
    "    count_dupe, removed_dupes = length_of_w2v_dupes(ind, dupe_indices, webhose_data)\n",
    "\n",
    "    # Save list of dupe indices\n",
    "    return removed_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Cycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\nCycling through 1000 articles and removing duplicates...\n"
    }
   ],
   "source": [
    "feed_length = len(feeds)\n",
    "unique_data = webhose_data\n",
    "for i in range(feed_length):\n",
    "    if i % 1000 == 0:\n",
    "        print('Cycling through 1000 articles and removing duplicates...')\n",
    "    \n",
    "    try:\n",
    "        removed_dupes = find_dupes(i, simhash_index, unique_data)\n",
    "        removed_dupes2 = [int(dupe) for dupe in removed_dupes]\n",
    "        unique_data = [feed for feed in unique_data if feed['id'] not in removed_dupes2]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the final dataset to a ```JSON``` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 9298 unique articles in this dataset.\n"
    }
   ],
   "source": [
    "with open('unique_data.json', 'w') as f:\n",
    "    json.dump(unique_data, f)\n",
    "\n",
    "print('There are {} unique articles in this dataset.'.format(len(unique_data)))"
   ]
  }
 ]
}