{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy NER Training\n",
    "### For a complete guide follow the link: https://spacy.io/usage/training#ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the built-in pipeline components and add them to the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4.812755051068962}\n",
      "Losses {'ner': 4.853534077922632}\n",
      "Losses {'ner': 3.8539207287249155}\n",
      "Losses {'ner': 3.6629106220789254}\n",
      "Losses {'ner': 5.162607038393617}\n",
      "Losses {'ner': 8.260472543537617}\n",
      "Losses {'ner': 10.034777522087097}\n",
      "Losses {'ner': 3.7499941085698083}\n",
      "Losses {'ner': 5.784585511544719}\n",
      "Losses {'ner': 4.448275563867355}\n",
      "Losses {'ner': 5.802332852035761}\n",
      "Losses {'ner': 5.6075634768931195}\n",
      "Losses {'ner': 3.8599217486516864}\n",
      "Losses {'ner': 4.465400483459234}\n",
      "Losses {'ner': 3.375863112974912}\n",
      "Losses {'ner': 1.5217506107146619}\n",
      "Losses {'ner': 5.579888354928698}\n",
      "Losses {'ner': 2.888812129647704}\n",
      "Losses {'ner': 5.2996954917907715}\n",
      "Losses {'ner': 0.21163272335797956}\n",
      "Losses {'ner': 3.030632863752544}\n",
      "Losses {'ner': 1.1248829059659329}\n",
      "Losses {'ner': 3.2261551576666534}\n",
      "Losses {'ner': 1.6767731559230015}\n",
      "Losses {'ner': 4.068039894627873}\n",
      "Losses {'ner': 1.1112780468117762}\n",
      "Losses {'ner': 2.984107021067757}\n",
      "Losses {'ner': 4.327911824580724}\n",
      "Losses {'ner': 0.03499928910183314}\n",
      "Losses {'ner': 0.15911962205082375}\n",
      "Losses {'ner': 0.00317678510918995}\n",
      "Losses {'ner': 0.010305628587957472}\n",
      "Losses {'ner': 0.10069868746671773}\n",
      "Losses {'ner': 0.0028156386614455187}\n",
      "Losses {'ner': 0.02806891981890658}\n",
      "Losses {'ner': 0.015001786280208762}\n",
      "Losses {'ner': 0.0036310682307885145}\n",
      "Losses {'ner': 0.0018453124994266545}\n",
      "Losses {'ner': 0.0011602272840613992}\n",
      "Losses {'ner': 1.446231551824914e-06}\n",
      "Losses {'ner': 3.0096182738103963}\n",
      "Losses {'ner': 0.0004025299543854999}\n",
      "Losses {'ner': 0.0017204133042715009}\n",
      "Losses {'ner': 0.0019499931380458357}\n",
      "Losses {'ner': 0.0005734502751959525}\n",
      "Losses {'ner': 0.005378927843448275}\n",
      "Losses {'ner': 0.0011089908156137473}\n",
      "Losses {'ner': 1.1379824668306355}\n",
      "Losses {'ner': 0.00022879376445136715}\n",
      "Losses {'ner': 0.024110023157721372}\n",
      "Losses {'ner': 1.3603344693191577e-06}\n",
      "Losses {'ner': 0.06970129642645645}\n",
      "Losses {'ner': 0.5647941796569071}\n",
      "Losses {'ner': 0.003764686782418203}\n",
      "Losses {'ner': 0.035634910596617206}\n",
      "Losses {'ner': 6.311698025340817e-09}\n",
      "Losses {'ner': 0.0007636258492524917}\n",
      "Losses {'ner': 0.0018893449899239773}\n",
      "Losses {'ner': 6.369691459749693e-06}\n",
      "Losses {'ner': 0.7710327623063691}\n",
      "Losses {'ner': 0.0001423302834661448}\n",
      "Losses {'ner': 1.0306019046860904}\n",
      "Losses {'ner': 0.0004380153491488348}\n",
      "Losses {'ner': 3.0853489374916415e-07}\n",
      "Losses {'ner': 7.59898533590865e-07}\n",
      "Losses {'ner': 7.125182639110077e-06}\n",
      "Losses {'ner': 4.1836064357328063e-07}\n",
      "Losses {'ner': 3.219948661068806e-07}\n",
      "Losses {'ner': 1.0531226813024925}\n",
      "Losses {'ner': 2.1675083361517855e-06}\n",
      "Losses {'ner': 0.0007366748314727549}\n",
      "Losses {'ner': 0.0040064597653050055}\n",
      "Losses {'ner': 0.010757259185725156}\n",
      "Losses {'ner': 1.2734707129091749e-08}\n",
      "Losses {'ner': 7.850980640981831e-07}\n",
      "Losses {'ner': 3.9258099479549946e-05}\n",
      "Losses {'ner': 2.4538954486444446e-06}\n",
      "Losses {'ner': 1.0503397068029963e-06}\n",
      "Losses {'ner': 7.369734311486871e-10}\n",
      "Losses {'ner': 1.0866344185667108e-07}\n",
      "Losses {'ner': 4.2382941014412756e-08}\n",
      "Losses {'ner': 0.031992182748229375}\n",
      "Losses {'ner': 0.0329904569138811}\n",
      "Losses {'ner': 3.6228620613383234e-05}\n",
      "Losses {'ner': 5.008282355528562e-06}\n",
      "Losses {'ner': 4.139503894102145e-08}\n",
      "Losses {'ner': 0.0008989023648770966}\n",
      "Losses {'ner': 1.8557235016215031e-06}\n",
      "Losses {'ner': 3.28028418358653e-05}\n",
      "Losses {'ner': 8.209218168439663e-09}\n",
      "Losses {'ner': 0.003969925309294305}\n",
      "Losses {'ner': 1.513756713422687e-06}\n",
      "Losses {'ner': 1.305794537757922e-05}\n",
      "Losses {'ner': 1.603409254370301e-05}\n",
      "Losses {'ner': 5.166856992628864e-08}\n",
      "Losses {'ner': 0.00027534792753271}\n",
      "Losses {'ner': 1.4186589841256945e-07}\n",
      "Losses {'ner': 4.6835096853063226e-12}\n",
      "Losses {'ner': 3.346985173327538e-06}\n",
      "Losses {'ner': 1.0987874581672356e-06}\n"
     ]
    }
   ],
   "source": [
    "n_iter=100\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    # reset and initialize the weights randomly â€“ but only if we're\n",
    "    # training a new model\n",
    "    #if model is None:\n",
    "    nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "        print(\"Losses\", losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 43.815919820765885}\n",
      "Statring iteration 1\n",
      "{'ner': 2.047680901964909}\n",
      "Statring iteration 2\n",
      "{'ner': 2.5135441263066483}\n",
      "Statring iteration 3\n",
      "{'ner': 1.376764312981628}\n",
      "Statring iteration 4\n",
      "{'ner': 6.008421308999736}\n",
      "Statring iteration 5\n",
      "{'ner': 4.474161270750194}\n",
      "Statring iteration 6\n",
      "{'ner': 2.489613622551231}\n",
      "Statring iteration 7\n",
      "{'ner': 4.875727091853037}\n",
      "Statring iteration 8\n",
      "{'ner': 5.0864285588152525}\n",
      "Statring iteration 9\n",
      "{'ner': 2.2597783342204742}\n",
      "Statring iteration 10\n",
      "{'ner': 1.750646549819676}\n",
      "Statring iteration 11\n",
      "{'ner': 1.918658120720616}\n",
      "Statring iteration 12\n",
      "{'ner': 0.25029540560569963}\n",
      "Statring iteration 13\n",
      "{'ner': 0.10027956947750176}\n",
      "Statring iteration 14\n",
      "{'ner': 0.03748057490395011}\n",
      "Statring iteration 15\n",
      "{'ner': 0.0005252629585224817}\n",
      "Statring iteration 16\n",
      "{'ner': 0.0012690736632870099}\n",
      "Statring iteration 17\n",
      "{'ner': 2.4943070916240167e-05}\n",
      "Statring iteration 18\n",
      "{'ner': 1.7767640544854049e-06}\n",
      "Statring iteration 19\n",
      "{'ner': 1.3094956470969505e-07}\n",
      "Enter your Model Name: prdname_ner\n",
      "Enter your testing text: Pandora makes the best earrings\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = [('what is the price of polo?', {'entities': [(21, 25, 'PrdName')]}), \n",
    "              ('what is the price of ball?', {'entities': [(21, 25, 'PrdName')]}), \n",
    "              ('what is the price of jegging?', {'entities': [(21, 28, 'PrdName')]}), \n",
    "              ('what is the price of t-shirt?', {'entities': [(21, 28, 'PrdName')]}), \n",
    "              ('what is the price of jeans?', {'entities': [(21, 26, 'PrdName')]}), \n",
    "              ('what is the price of bat?', {'entities': [(21, 24, 'PrdName')]}), \n",
    "              ('what is the price of shirt?', {'entities': [(21, 26, 'PrdName')]}), \n",
    "              ('what is the price of bag?', {'entities': [(21, 24, 'PrdName')]}), \n",
    "              ('what is the price of cup?', {'entities': [(21, 24, 'PrdName')]}), \n",
    "              ('what is the price of jug?', {'entities': [(21, 24, 'PrdName')]}), \n",
    "              ('what is the price of plate?', {'entities': [(21, 26, 'PrdName')]}), \n",
    "              ('what is the price of glass?', {'entities': [(21, 26, 'PrdName')]}), \n",
    "              ('what is the price of moniter?', {'entities': [(21, 28, 'PrdName')]}), \n",
    "              ('what is the price of desktop?', {'entities': [(21, 28, 'PrdName')]}), \n",
    "              ('what is the price of bottle?', {'entities': [(21, 27, 'PrdName')]}), \n",
    "              ('what is the price of mouse?', {'entities': [(21, 26, 'PrdName')]}), \n",
    "              ('what is the price of keyboad?', {'entities': [(21, 28, 'PrdName')]}), \n",
    "              ('what is the price of chair?', {'entities': [(21, 26, 'PrdName')]}), \n",
    "              ('what is the price of table?', {'entities': [(21, 26, 'PrdName')]}), \n",
    "              ('what is the price of watch?', {'entities': [(21, 26, 'PrdName')]})]\n",
    "\n",
    "\n",
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "prdnlp = train_spacy(TRAIN_DATA, 20)\n",
    "\n",
    "# Save our trained Model\n",
    "modelfile = input(\"Enter your Model Name: \")\n",
    "prdnlp.to_disk(modelfile)\n",
    "\n",
    "#Test your text\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "doc = prdnlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
